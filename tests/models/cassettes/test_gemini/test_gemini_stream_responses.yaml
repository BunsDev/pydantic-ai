interactions:
- request:
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '86'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
    method: POST
    parsed_body:
      contents:
      - parts:
        - text: Hello
        role: user
      generationConfig: {}
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:streamGenerateContent?alt=sse
  response:
    body:
      string: "data: {\"candidates\": [{\"content\": {\"parts\": [{\"text\": \"Hi\"}],\"role\": \"model\"}}],\"usageMetadata\":
        {\"promptTokenCount\": 2,\"totalTokenCount\": 2,\"promptTokensDetails\": [{\"modality\": \"TEXT\",\"tokenCount\":
        2}]},\"modelVersion\": \"gemini-2.0-flash-exp\"}\r\n\r\ndata: {\"candidates\": [{\"content\": {\"parts\": [{\"text\":
        \" there! How can I help you today?\\n\"}],\"role\": \"model\"},\"finishReason\": \"STOP\"}],\"usageMetadata\": {\"promptTokenCount\":
        1,\"candidatesTokenCount\": 11,\"totalTokenCount\": 12,\"promptTokensDetails\": [{\"modality\": \"TEXT\",\"tokenCount\":
        1}],\"candidatesTokensDetails\": [{\"modality\": \"TEXT\",\"tokenCount\": 11}]},\"modelVersion\": \"gemini-2.0-flash-exp\"}\r\n\r\n"
    headers:
      alt-svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      content-disposition:
      - attachment
      content-type:
      - text/event-stream
      server-timing:
      - gfet4t7; dur=595
      transfer-encoding:
      - chunked
      vary:
      - Origin
      - X-Origin
      - Referer
    status:
      code: 200
      message: OK
version: 1
